run:
  name: convert_${conversion.run.model_train_name}
  nodes: ${divide_ceil:${conversion.model.model_parallel_size}, 8} # 8 gpus per node
  time_limit: "2:00:00"
  dependency: "singleton"
  ntasks_per_node: ${divide_ceil:${conversion.model.model_parallel_size}, ${.nodes}}
  convert_name: convert_nemo
  model_train_name: t5_220m
  train_dir: ${base_results_dir}/${.model_train_name}
  results_dir: ${.train_dir}/${.convert_name}
  nemo_file_name: megatron_t5.nemo # name of nemo checkpoint; must be .nemo file
  pack_nemo_file: True # true to compress as a .nemo file, false to write files under nemo_file_name as a directory

model:
  model_type: t5 # gpt or t5, use t5 for mt5 as well
  checkpoint_folder: ${conversion.run.train_dir}/results/checkpoints
  checkpoint_name: latest # latest OR name pattern of a checkpoint (e.g. megatron_gpt-*last.ckpt)
  hparams_file: ${conversion.run.train_dir}/results/hparams.yaml
  tensor_model_parallel_size: 1 # 1 for 220m, 2 for 3b
  pipeline_model_parallel_size: 1
  pipeline_model_parallel_split_rank: ${divide_floor:${.pipeline_model_parallel_size}, 2}
  model_parallel_size: ${multiply:${.tensor_model_parallel_size}, ${.pipeline_model_parallel_size}}
  vocab_file: ${data_dir}/bpe/vocab.txt
  merge_file: null
