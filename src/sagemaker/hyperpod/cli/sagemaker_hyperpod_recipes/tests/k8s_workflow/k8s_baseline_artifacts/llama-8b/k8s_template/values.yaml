image:
  trainingImage: test_container
  pullPolicy: Always
trainingConfig:
  jobName: llama-8b
  namespace: default
  scriptPath: examples/llama/llama_pretrain.py
  scriptArgs: --config-path=/config --config-name=config.yaml
  customScript: null
  annotations: null
  customLabels: null
  priority_class_name: null
  device: gpu
  numEFADevices: 32
  numNeuronDevices: null
  ntasksPerNode: 8
  nodes: 16
  restartPolicy: Never
  wandbKey: nil
  serviceAccountName: null
  compile: 0
  persistentVolumeClaims:
  - null
  volumes: null
  git:
    repo_url_or_path: https://test_token@github.com/aws/sagemaker-hyperpod-training-adapter-for-nemo.git
    branch: test_branch
    commit: test_commit
    token: null
    update_adapter: false
  pre_script: []
  post_script: []
  labelSelector:
    required: null
    preferred: null
    weights: null
  cleanPodPolicy: null
  envVars:
    NCCL_DEBUG: WARN
    NEMO_LAUNCHER_DEBUG: 1
    SLURM_NTASKS_PER_NODE: 8
    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
    FI_PROVIDER: efa
    NCCL_SOCKET_IFNAME: ^lo,docker0,veth_def_agent
    NCCL_IGNORE_DISABLED_P2P: '1'
    TORCH_NCCL_ASYNC_ERROR_HANDLING: '1'
    TORCH_DIST_INIT_BARRIER: '1'
    CUDA_DEVICE_MAX_CONNECTIONS: '1'
