{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d275612",
   "metadata": {},
   "source": [
    "## Inference Operator CLI E2E Expereience (JumpStart model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ed187",
   "metadata": {},
   "source": [
    "Make sure you have installed pacakges:\n",
    "- sagemaker-hyperpod\n",
    "- jumpstart_inference_config_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30debba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hyperpod list-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dcd2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hyperpod set-cluster-context --cluster-name hp-cluster-for-inf-Beta2try1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hyp create hyp-jumpstart-endpoint \\\n",
    "  --version 1.0 \\\n",
    "  --model-id deepseek-llm-r1-distill-qwen-1-5b \\\n",
    "  --model-version 2.0.4 \\\n",
    "  --instance-type ml.g5.8xlarge \\\n",
    "  --endpoint-name endpoint-test-jscli \\\n",
    "  --tls-certificate-output-s3-uri s3://tls-bucket-inf1-beta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d444d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hyp invoke hyp-jumpstart-endpoint --endpoint-name endpoint-test-jscli --body '{\"inputs\":\"What is the capital of USA?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c32fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hyp list hyp-jumpstart-endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hyp describe hyp-jumpstart-endpoint --name deepseek-llm-r1-distill-qwen-1-5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hyp delete hyp-jumpstart-endpoint --name deepseek-llm-r1-distill-qwen-1-5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae67bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hyp get-operator-logs hyp-jumpstart-endpoint --since-hours 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf2161f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mollyhe/.pyenv/versions/3.12.2/lib/python3.12/site-packages/pydantic/_internal/_fields.py:198: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n",
      "2025-07-01T19:02:02.679202231Z \u001b[2m2025-07-01T19:02:02.679061Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Args {\n",
      "2025-07-01T19:02:02.679226581Z     model_id: \"/opt/ml/model\",\n",
      "2025-07-01T19:02:02.679230201Z     revision: None,\n",
      "2025-07-01T19:02:02.679233021Z     validation_workers: 2,\n",
      "2025-07-01T19:02:02.679235001Z     sharded: None,\n",
      "2025-07-01T19:02:02.679236961Z     num_shard: None,\n",
      "2025-07-01T19:02:02.679238941Z     quantize: None,\n",
      "2025-07-01T19:02:02.679241091Z     speculate: None,\n",
      "2025-07-01T19:02:02.679243041Z     dtype: None,\n",
      "2025-07-01T19:02:02.679245241Z     trust_remote_code: false,\n",
      "2025-07-01T19:02:02.679247831Z     max_concurrent_requests: 128,\n",
      "2025-07-01T19:02:02.679249781Z     max_best_of: 2,\n",
      "2025-07-01T19:02:02.679251721Z     max_stop_sequences: 4,\n",
      "2025-07-01T19:02:02.679253642Z     max_top_n_tokens: 5,\n",
      "2025-07-01T19:02:02.679255732Z     max_input_tokens: None,\n",
      "2025-07-01T19:02:02.679257692Z     max_input_length: None,\n",
      "2025-07-01T19:02:02.679259692Z     max_total_tokens: None,\n",
      "2025-07-01T19:02:02.679261612Z     waiting_served_ratio: 0.3,\n",
      "2025-07-01T19:02:02.679263682Z     max_batch_prefill_tokens: None,\n",
      "2025-07-01T19:02:02.679265682Z     max_batch_total_tokens: None,\n",
      "2025-07-01T19:02:02.679267772Z     max_waiting_tokens: 20,\n",
      "2025-07-01T19:02:02.679269692Z     max_batch_size: None,\n",
      "2025-07-01T19:02:02.679271702Z     cuda_graphs: None,\n",
      "2025-07-01T19:02:02.679274232Z     hostname: \"deepseek15b-test-model-name-07-01-2-6588f764c-5lxwk\",\n",
      "2025-07-01T19:02:02.679276232Z     port: 8080,\n",
      "2025-07-01T19:02:02.679278322Z     shard_uds_path: \"/tmp/text-generation-server\",\n",
      "2025-07-01T19:02:02.679280382Z     master_addr: \"localhost\",\n",
      "2025-07-01T19:02:02.679282302Z     master_port: 29500,\n",
      "2025-07-01T19:02:02.679284242Z     huggingface_hub_cache: None,\n",
      "2025-07-01T19:02:02.679286582Z     weights_cache_override: None,\n",
      "2025-07-01T19:02:02.679288612Z     disable_custom_kernels: false,\n",
      "2025-07-01T19:02:02.679290682Z     cuda_memory_fraction: 1.0,\n",
      "2025-07-01T19:02:02.679292602Z     rope_scaling: None,\n",
      "2025-07-01T19:02:02.679294762Z     rope_factor: None,\n",
      "2025-07-01T19:02:02.679297272Z     json_output: false,\n",
      "2025-07-01T19:02:02.679299742Z     otlp_endpoint: None,\n",
      "2025-07-01T19:02:02.679302362Z     otlp_service_name: \"text-generation-inference.router\",\n",
      "2025-07-01T19:02:02.679304972Z     cors_allow_origin: [],\n",
      "2025-07-01T19:02:02.679307472Z     api_key: None,\n",
      "2025-07-01T19:02:02.679310052Z     watermark_gamma: None,\n",
      "2025-07-01T19:02:02.679312592Z     watermark_delta: None,\n",
      "2025-07-01T19:02:02.679315112Z     ngrok: false,\n",
      "2025-07-01T19:02:02.679317352Z     ngrok_authtoken: None,\n",
      "2025-07-01T19:02:02.679319462Z     ngrok_edge: None,\n",
      "2025-07-01T19:02:02.679321402Z     tokenizer_config_path: None,\n",
      "2025-07-01T19:02:02.679323452Z     disable_grammar_support: false,\n",
      "2025-07-01T19:02:02.679325383Z     env: false,\n",
      "2025-07-01T19:02:02.679327453Z     max_client_batch_size: 4,\n",
      "2025-07-01T19:02:02.679329373Z     lora_adapters: None,\n",
      "2025-07-01T19:02:02.679331393Z     usage_stats: On,\n",
      "2025-07-01T19:02:02.679333323Z }\n",
      "2025-07-01T19:02:02.744133905Z \u001b[2m2025-07-01T19:02:02.744024Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Model supports up to 131072 but tgi will now set its default to 4096 instead. This is to save VRAM by refusing large prompts in order to allow more users on the same hardware. You can increase that size using `--max-batch-prefill-tokens=131122 --max-total-tokens=131072 --max-input-tokens=131071`.\n",
      "2025-07-01T19:02:04.216374922Z \u001b[2m2025-07-01T19:02:04.216234Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Using attention flashinfer - Prefix caching true\n",
      "2025-07-01T19:02:04.216423602Z \u001b[2m2025-07-01T19:02:04.216270Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_input_tokens` to 4095\n",
      "2025-07-01T19:02:04.216429482Z \u001b[2m2025-07-01T19:02:04.216273Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_total_tokens` to 4096\n",
      "2025-07-01T19:02:04.216432902Z \u001b[2m2025-07-01T19:02:04.216275Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Default `max_batch_prefill_tokens` to 4145\n",
      "2025-07-01T19:02:04.216435952Z \u001b[2m2025-07-01T19:02:04.216278Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Using default cuda graphs [1, 2, 4, 8, 16, 32]\n",
      "2025-07-01T19:02:04.216568454Z \u001b[2m2025-07-01T19:02:04.216469Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mdownload\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting check and download process for /opt/ml/model\n",
      "2025-07-01T19:02:07.211964285Z \u001b[2m2025-07-01T19:02:07.211826Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Files are already present on the host. Skipping download.\n",
      "2025-07-01T19:02:07.834603321Z \u001b[2m2025-07-01T19:02:07.834468Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mdownload\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Successfully downloaded weights for /opt/ml/model\n",
      "2025-07-01T19:02:07.834835164Z \u001b[2m2025-07-01T19:02:07.834716Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting shard \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "2025-07-01T19:02:10.931611886Z \u001b[2m2025-07-01T19:02:10.931516Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Using prefix caching = True\n",
      "2025-07-01T19:02:10.931641296Z \u001b[2m2025-07-01T19:02:10.931552Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Using Attention = flashinfer\n",
      "2025-07-01T19:02:17.857700061Z \u001b[2m2025-07-01T19:02:17.857530Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Waiting for shard to be ready... \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "2025-07-01T19:02:19.770598947Z \u001b[2m2025-07-01T19:02:19.770462Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Server started at unix:///tmp/text-generation-server-0\n",
      "2025-07-01T19:02:19.859479084Z \u001b[2m2025-07-01T19:02:19.859373Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mshard-manager\u001b[0m: \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Shard ready in 12.010886696s \u001b[2m\u001b[3mrank\u001b[0m\u001b[2m=\u001b[0m0\u001b[0m\n",
      "2025-07-01T19:02:19.944559677Z \u001b[2m2025-07-01T19:02:19.944402Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Starting Webserver\n",
      "2025-07-01T19:02:20.009769348Z \u001b[2m2025-07-01T19:02:20.009584Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router_v3\u001b[0m\u001b[2m:\u001b[0m \u001b[2mbackends/v3/src/lib.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m90:\u001b[0m Warming up model\n",
      "2025-07-01T19:02:20.699970797Z \u001b[2m2025-07-01T19:02:20.699808Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_launcher\u001b[0m\u001b[2m:\u001b[0m Cuda Graphs are enabled for sizes [32, 16, 8, 4, 2, 1]\n",
      "2025-07-01T19:02:21.696696179Z \u001b[2m2025-07-01T19:02:21.696510Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router_v3\u001b[0m\u001b[2m:\u001b[0m \u001b[2mbackends/v3/src/lib.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m102:\u001b[0m Setting max batch total tokens to 544704\n",
      "2025-07-01T19:02:21.696738040Z \u001b[2m2025-07-01T19:02:21.696558Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router_v3\u001b[0m\u001b[2m:\u001b[0m \u001b[2mbackends/v3/src/lib.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m127:\u001b[0m Using backend V3\n",
      "2025-07-01T19:02:25.693422977Z \u001b[2m2025-07-01T19:02:25.693143Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router::server\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/server.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m1670:\u001b[0m Using config Some(Qwen2)\n",
      "2025-07-01T19:02:25.921792454Z \u001b[2m2025-07-01T19:02:25.921617Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtext_generation_router::server\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/server.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m1715:\u001b[0m no pipeline tag found for model /opt/ml/model\n",
      "2025-07-01T19:02:25.921834315Z \u001b[2m2025-07-01T19:02:25.921647Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtext_generation_router::server\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/server.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m1817:\u001b[0m Invalid hostname, defaulting to 0.0.0.0\n",
      "2025-07-01T19:02:26.018292054Z \u001b[2m2025-07-01T19:02:26.018128Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router::server\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/server.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m2210:\u001b[0m Connected\n",
      "2025-07-01T19:12:33.291278507Z \u001b[2m2025-07-01T19:12:33.291107Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router_v3::radix\u001b[0m\u001b[2m:\u001b[0m \u001b[2mbackends/v3/src/radix.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m108:\u001b[0m Prefix 0 - Suffix 107\n",
      "2025-07-01T19:12:33.750403282Z \u001b[2m2025-07-01T19:12:33.750213Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mcompat_generate\u001b[0m\u001b[1m{\u001b[0m\u001b[3mdefault_return_full_text\u001b[0m\u001b[2m=\u001b[0mtrue \u001b[3mcompute_type\u001b[0m\u001b[2m=\u001b[0mExtension(ComputeType(\"1-nvidia-a10g\"))\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m\u001b[1mgenerate\u001b[0m\u001b[1m{\u001b[0m\u001b[3mparameters\u001b[0m\u001b[2m=\u001b[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, frequency_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: true, max_new_tokens: Some(100), return_full_text: Some(true), stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None, grammar: None, adapter_id: None } \u001b[3mtotal_time\u001b[0m\u001b[2m=\u001b[0m\"459.555412ms\" \u001b[3mvalidation_time\u001b[0m\u001b[2m=\u001b[0m\"420.246µs\" \u001b[3mqueue_time\u001b[0m\u001b[2m=\u001b[0m\"79.181µs\" \u001b[3minference_time\u001b[0m\u001b[2m=\u001b[0m\"459.056175ms\" \u001b[3mtime_per_token\u001b[0m\u001b[2m=\u001b[0m\"9.181123ms\" \u001b[3mseed\u001b[0m\u001b[2m=\u001b[0m\"Some(15067716292412336081)\"\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m \u001b[2mtext_generation_router::server\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/server.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m402:\u001b[0m Success\n",
      "2025-07-01T20:17:27.201894201Z \u001b[2m2025-07-01T20:17:27.201733Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_generation_router_v3::radix\u001b[0m\u001b[2m:\u001b[0m \u001b[2mbackends/v3/src/radix.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m108:\u001b[0m Prefix 6 - Suffix 101\n",
      "2025-07-01T20:17:28.112421812Z \u001b[2m2025-07-01T20:17:28.112251Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mcompat_generate\u001b[0m\u001b[1m{\u001b[0m\u001b[3mdefault_return_full_text\u001b[0m\u001b[2m=\u001b[0mtrue \u001b[3mcompute_type\u001b[0m\u001b[2m=\u001b[0mExtension(ComputeType(\"1-nvidia-a10g\"))\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m\u001b[1mgenerate\u001b[0m\u001b[1m{\u001b[0m\u001b[3mparameters\u001b[0m\u001b[2m=\u001b[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, frequency_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: true, max_new_tokens: Some(100), return_full_text: Some(true), stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None, grammar: None, adapter_id: None } \u001b[3mtotal_time\u001b[0m\u001b[2m=\u001b[0m\"910.811145ms\" \u001b[3mvalidation_time\u001b[0m\u001b[2m=\u001b[0m\"266.384µs\" \u001b[3mqueue_time\u001b[0m\u001b[2m=\u001b[0m\"70.331µs\" \u001b[3minference_time\u001b[0m\u001b[2m=\u001b[0m\"910.47459ms\" \u001b[3mtime_per_token\u001b[0m\u001b[2m=\u001b[0m\"9.104745ms\" \u001b[3mseed\u001b[0m\u001b[2m=\u001b[0m\"Some(11348950285833071996)\"\u001b[1m}\u001b[0m\u001b[2m:\u001b[0m \u001b[2mtext_generation_router::server\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/server.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m402:\u001b[0m Success\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hyp get-logs hyp-jumpstart-endpoint --pod-name <pod-name>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
