model_id: "Qwen/Qwen3-4B-Thinking-2507" # Hugging Face model id
mlflow_uri: "" # MLflow tracking server URI
mlflow_experiment_name: "" # MLflow experiment name
# sagemaker specific parameters
output_dir: "/data/qwen-cli-example/model/" # path to where SageMaker will upload the model
train_dataset_path: "/data/qwen-cli-example/dataset/train/" # path to train dataset in FSx
val_dataset_path: "/data/qwen-cli-example/dataset/val/" # path to where test dataset in FSx saves
checkpoint_dir: "/data/qwen-cli-example/checkpoints/" # path to where FSx saves the checkpoints
token: "" # Hugging Face API token
merge_weights: true # merge weights in the base model
# training parameters
use_checkpoints: true
apply_truncation: true # apply truncation to datasets
use_snapshot_download: false # Use snapshot_download to download the model
attn_implementation: "flash_attention_2" # attention implementation type
lr_scheduler_type: "cosine" # learning rate scheduler type
learning_rate: 2e-5 # learning rate scheduler
num_train_epochs: 1000000 # number of training epochs
per_device_train_batch_size: 2 # batch size per device during training
per_device_eval_batch_size: 1 # batch size for evaluation
gradient_accumulation_steps: 4 # number of steps before performing a backward/update pass
gradient_checkpointing: true # use gradient checkpointing
torch_dtype: "bfloat16" # float precision type
bf16: true # use bfloat16 precision
tf32: true # use tf32 precision
ignore_data_skip: true # skip data loading errors
logging_strategy: "steps" # logging strategy
logging_steps: 1 # log every N steps
log_on_each_node: false # disable logging on each node
ddp_find_unused_parameters: false # DDP unused parameter detection
save_total_limit: 1 # maximum number of checkpoints to keep
save_steps: 8 # Save checkpoint every this many steps
warmup_steps: 3 # number of warmup steps
weight_decay: 0.01 # weight decay coefficient
eval_strategy: "steps" # Add evaluation
eval_steps: 17 # Evaluate every ~half epoch
fsdp: "full_shard auto_wrap offload" # FSDP sharding strategy
fsdp_config: # FSDP configuration options
  backward_prefetch: "backward_pre" # prefetch parameters during backward pass
  cpu_ram_efficient_loading: true # enable CPU RAM efficient model loading
  offload_params: false # offload parameters to CPU
  forward_prefetch: false # disable forward prefetch
  use_orig_params: false # use original parameter names
# LoRA parameters
load_in_4bit: true # enable 4-bit quantization
use_mxfp4: false # use MXFP4 quantization
lora_r: 16 # LoRA rank
lora_alpha: 32 # LoRA alpha parameter
lora_dropout: 0.1 # LoRA dropout rate
